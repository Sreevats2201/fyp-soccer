{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5400, 2048)\n",
      "(5400, 512)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "pca = PCA(n_components=512)\n",
    "array = np.load('1_ResNET.npy')\n",
    "print(array.shape)\n",
    "pca_arr = pca.fit_transform(array)\n",
    "np.save('1_ResNET_PCA512.npy', pca_arr)\n",
    "print(pca_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4630162  0.06204205 4.4352226  ... 1.5256827  3.5007942  0.        ]\n",
      " [1.3390025  0.05157435 4.769183   ... 0.67852765 1.8540585  0.21820806]\n",
      " [0.53878206 0.01563736 5.3364444  ... 0.36709633 3.5641797  0.01204965]\n",
      " ...\n",
      " [0.         0.13242404 4.908689   ... 2.4691331  1.3422186  0.20065752]\n",
      " [0.         0.08716917 4.3659477  ... 2.1971464  1.4559647  0.1773672 ]\n",
      " [0.         0.07030997 3.5072024  ... 3.0104277  0.9120427  0.00837918]] (5400, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(array, array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.8080301e+00  1.2697511e+01  5.8431826e+00 ...  1.7071088e-01\n",
      "  -2.3633674e-02  3.3422112e-01]\n",
      " [-9.2166674e-01  1.8923967e+01  2.7841573e+00 ...  7.1200885e-02\n",
      "  -3.5260969e-01  4.4601795e-01]\n",
      " [ 8.0361170e-01  1.9779863e+01 -2.2374811e+00 ... -3.4451136e-01\n",
      "  -7.1129203e-02  2.3810212e-01]\n",
      " ...\n",
      " [-1.2372240e+01 -3.9151075e-01 -2.0314646e+01 ... -3.5327846e-01\n",
      "   1.6631913e-01 -1.1573656e-01]\n",
      " [-1.4105256e+01 -2.3478653e+00 -1.8007633e+01 ... -2.3208618e-01\n",
      "   2.5485778e-02 -8.6775243e-02]\n",
      " [-1.3138857e+01  8.7422854e-01 -1.7679382e+01 ...  1.4730068e-01\n",
      "   2.8727269e-01 -2.3334429e-03]] (5400, 512)\n"
     ]
    }
   ],
   "source": [
    "print(pca_arr, pca_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Defining the folders and the file names to import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_FOLDER = 'data/features/'\n",
    "LABEL_FOLDER = 'data/labels'\n",
    "\n",
    "FEAT_FILE_1 = '1_ResNET_PCA512.npy'\n",
    "FEAT_FILE_2 = '2_ResNET_PCA512.npy'\n",
    "\n",
    "FEAT_FILE = 'feat.pickle'\n",
    "LABEL_FILE = 'label.pickle'\n",
    "\n",
    "MODEL = r'C:\\Users\\ranja\\Desktop\\Notebooks\\SoccerNet-code-master\\SoccerNet-code-master\\src\\feature_extraction\\model.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the contents of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['england_epl',\n",
       " 'europe_uefa-champions-league',\n",
       " 'france_ligue-1',\n",
       " 'germany_bundesliga',\n",
       " 'italy_serie-a',\n",
       " 'spain_laliga']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(LABEL_FOLDER) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating en empty list that will hold the locations of <br>\n",
    " - all the features extracted\n",
    " - all the labels parsed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FEAT_FOLD_LIST = []\n",
    "LABEL_FOLD_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Feature and Label Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a tree walk through the features folder, to find all the folders that contain the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root, subdirs, files in os.walk(FEAT_FOLDER):\n",
    "    if files:\n",
    "        for f in files:\n",
    "            if f.endswith('.npy') and root not in FEAT_FOLD_LIST:\n",
    "                FEAT_FOLD_LIST += [root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/features/england_epl\\\\2014-2015\\\\2015-02-21 - 18-00 Chelsea 1 - 1 Burnley',\n",
       " 'data/features/england_epl\\\\2014-2015\\\\2015-02-21 - 18-00 Crystal Palace 1 - 2 Arsenal']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEAT_FOLD_LIST[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing a tree walk through the labels folder, to find all the folders that contain the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for root, subdirs, files in os.walk(LABEL_FOLDER):\n",
    "    if files:\n",
    "        for f in files:\n",
    "            if f.endswith('.json') and root not in LABEL_FOLD_LIST:\n",
    "                LABEL_FOLD_LIST += [root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/labels\\\\england_epl\\\\2014-2015\\\\2015-02-21 - 18-00 Chelsea 1 - 1 Burnley',\n",
       " 'data/labels\\\\england_epl\\\\2014-2015\\\\2015-02-21 - 18-00 Crystal Palace 1 - 2 Arsenal']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL_FOLD_LIST[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding features for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing certain static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to determine how many events to iterate through\n",
    "FILE_LEN = len(FEAT_FOLD_LIST)\n",
    "\n",
    "#Type of Events taken from labels and assigned to variables\n",
    "GOAL = \"soccer-ball\"\n",
    "FOUL = \"y-card\"\n",
    "SUBS = \"substitution-in\"\n",
    "NO_EVENT = \"no\"\n",
    "\n",
    "#This variable determines how many features around the index we want to take\n",
    "ws = 10 #5 second window, 5/60 * 2 * 60 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inititalizing features, making them 512 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GOAL_FEATURES = np.empty([1, 512])\n",
    "FOUL_FEATURES = np.empty([1, 512])\n",
    "SUBS_FEATURES = np.empty([1, 512])\n",
    "NO_FEATURES = np.empty([1, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    function : to find and give back the combined set of resnet features\n",
    "    input : the location of the features\n",
    "    output : features for the entire match\n",
    "    \"\"\"\n",
    "    \n",
    "    files = os.listdir(folder)\n",
    "    if FEAT_FILE_1 not in files or FEAT_FILE_2 not in files:\n",
    "        return None\n",
    "    first_half = np.load(folder + '/' + FEAT_FILE_1)\n",
    "    second_half = np.load(folder + '/' + FEAT_FILE_2)\n",
    "    return np.concatenate((first_half, second_half))\n",
    "\n",
    "def get_labels(folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    function : to find the labels and return\n",
    "    inpupt : location of labels\n",
    "    output : labels in json format\n",
    "    \"\"\"\n",
    "    \n",
    "    file = os.listdir(folder)\n",
    "    with open(folder + '/' + file[0]) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def parse_labels(labels, size):\n",
    "    \n",
    "    \"\"\"\n",
    "    function : to return the array of indices of every event type\n",
    "    input : labels in json format and the size of the features (length)\n",
    "    output : a dictionary containing an array of values for every event type\n",
    "    \"\"\"\n",
    "    \n",
    "    annots = labels['annotations']\n",
    "    \n",
    "    dict_events = {}\n",
    "    goal_time = []\n",
    "    foul_time = []\n",
    "    subs_time = []\n",
    "    \n",
    "    for i in range(len(annots)):\n",
    "        if annots[i]['label'] == GOAL:\n",
    "            time = annots[i]['gameTime']\n",
    "            vals = time.split('-')\n",
    "            final_time = int(vals[0]) * int(vals[1][0:3])\n",
    "            final_time = final_time * 120\n",
    "            if final_time > size:\n",
    "                final_time = size - ws\n",
    "            goal_time += [final_time]\n",
    "\n",
    "        if annots[i]['label'] == FOUL:\n",
    "            time = annots[i]['gameTime']\n",
    "            vals = time.split('-')\n",
    "            final_time = int(vals[0]) * int(vals[1][0:3])\n",
    "            final_time = final_time * 120\n",
    "            if final_time > size:\n",
    "                final_time = size - ws\n",
    "            foul_time += [final_time]\n",
    "            \n",
    "        if annots[i]['label'] == SUBS:\n",
    "            time = annots[i]['gameTime']\n",
    "            vals = time.split('-')\n",
    "            final_time = int(vals[0]) * int(vals[1][0:3])\n",
    "            final_time = final_time * 120\n",
    "            if final_time > size:\n",
    "                final_time = size - ws            \n",
    "            subs_time += [final_time]\n",
    "            \n",
    "    dict_events[GOAL] = goal_time\n",
    "    dict_events[FOUL] = foul_time\n",
    "    dict_events[SUBS] = subs_time\n",
    "    return dict_events\n",
    "\n",
    "def interpolate(arr, ws):\n",
    "    \n",
    "    \"\"\"\n",
    "    function : to find features on either side of the value\n",
    "    input : the array of values and the window size\n",
    "    output : modified array of indices\n",
    "    \"\"\"\n",
    "    \n",
    "    new_arr = [list(range(x-ws, x+1)) for x in arr]\n",
    "    merged_list = []\n",
    "\n",
    "    for l in new_arr:\n",
    "        merged_list += l\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating features of a particular event into respective arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 234/234 [00:23<00:00,  9.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(FILE_LEN)):\n",
    "    feat_dir = FEAT_FOLD_LIST[i]\n",
    "    label_dir = LABEL_FOLD_LIST[i]\n",
    "    \n",
    "    features = get_features(feat_dir)\n",
    "    if features is None:\n",
    "        continue\n",
    "        \n",
    "    features_small = features[0:100]\n",
    "    feat_shape = list(range(features.shape[0]))[0:100]\n",
    "    \n",
    "    labels = get_labels(label_dir)\n",
    "    \n",
    "    events = parse_labels(labels, features.shape[0])\n",
    "    \n",
    "    g = events[GOAL]\n",
    "    f = events[FOUL]\n",
    "    s = events[SUBS]\n",
    "    \n",
    "    num = interpolate(g, ws) + interpolate(f, ws) + interpolate(s, ws)\n",
    "    #print(num) \n",
    "    \n",
    "    y = [not x in num for x in feat_shape]\n",
    "    \n",
    "    GOAL_FEATURES = np.concatenate([GOAL_FEATURES, features[interpolate(g, ws)]])\n",
    "    FOUL_FEATURES = np.concatenate([FOUL_FEATURES, features[interpolate(f, ws)]])\n",
    "    SUBS_FEATURES = np.concatenate([SUBS_FEATURES, features[interpolate(s, ws)]])\n",
    "    NO_FEATURES = np.concatenate([NO_FEATURES, features_small[y]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7239, 512) (8856, 512) (13619, 512) (22018, 512)\n"
     ]
    }
   ],
   "source": [
    "print(GOAL_FEATURES.shape, FOUL_FEATURES.shape, SUBS_FEATURES.shape, NO_FEATURES.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Train and Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28956, 512) (28956,)\n"
     ]
    }
   ],
   "source": [
    "# Find the minimum length of all the three event feature types\n",
    "n = min(GOAL_FEATURES.shape[0], FOUL_FEATURES.shape[0], SUBS_FEATURES.shape[0], NO_FEATURES.shape[0])\n",
    "\n",
    "#Create target class variable of length n\n",
    "g_class = [GOAL] * n\n",
    "f_class = [FOUL] * n\n",
    "s_class = [SUBS] * n\n",
    "n_class = [NO_EVENT] *n\n",
    "\n",
    "#Ensure that the train features and target labels are of the same length\n",
    "TRAIN_FEATURES = np.concatenate([GOAL_FEATURES[:n], FOUL_FEATURES[:n], SUBS_FEATURES[:n], NO_FEATURES[:n]])\n",
    "#TRAIN_FEATURES = np.concatenate([GOAL_FEATURES[:n], FOUL_FEATURES[:n], SUBS_FEATURES[:n]])\n",
    "\n",
    "TARGET_LABELS = np.concatenate([g_class, f_class, s_class, n_class])\n",
    "#TARGET_LABELS = np.concatenate([g_class, f_class, s_class])\n",
    "\n",
    "print(TRAIN_FEATURES.shape, TARGET_LABELS.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_out = open(FEAT_FILE, \"wb\")\n",
    "\n",
    "pickle.dump(TRAIN_FEATURES, feat_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_in = open(FEAT_FILE,\"rb\")\n",
    "\n",
    "TRAIN_FEATURES = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Enconding and Test Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the target varibles to make it suitable for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28956, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(TARGET_LABELS)\n",
    "encoded_Y = encoder.transform(TARGET_LABELS)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "y = np_utils.to_categorical(encoded_Y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(TRAIN_FEATURES, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #512 inputs to 8 neurons\n",
    "    model.add(Dense(19, input_dim=512, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(12, input_dim=19, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(7, input_dim=12, activation='relu'))\n",
    "\n",
    "    #Final 4 categories, so again three dense layers\n",
    "    model.add(Dense(4))\n",
    "    \n",
    "    #Add softmax activation function\n",
    "    model.add(Activation(tf.nn.softmax))\n",
    "    \n",
    "    # Compile model\n",
    "    \n",
    "    adam = optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of the model, and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23164/23164 [==============================] - ETA: 2s - loss: 1.2185 - acc: 0.375 - ETA: 1s - loss: 1.0609 - acc: 0.505 - ETA: 1s - loss: 1.0594 - acc: 0.508 - ETA: 1s - loss: 1.0676 - acc: 0.502 - ETA: 0s - loss: 1.0665 - acc: 0.505 - ETA: 0s - loss: 1.0605 - acc: 0.506 - ETA: 0s - loss: 1.0632 - acc: 0.501 - ETA: 0s - loss: 1.0615 - acc: 0.504 - ETA: 0s - loss: 1.0671 - acc: 0.501 - ETA: 0s - loss: 1.0711 - acc: 0.499 - ETA: 0s - loss: 1.0708 - acc: 0.498 - ETA: 0s - loss: 1.0687 - acc: 0.499 - ETA: 0s - loss: 1.0656 - acc: 0.502 - ETA: 0s - loss: 1.0628 - acc: 0.503 - ETA: 0s - loss: 1.0613 - acc: 0.505 - ETA: 0s - loss: 1.0589 - acc: 0.506 - ETA: 0s - loss: 1.0594 - acc: 0.505 - ETA: 0s - loss: 1.0589 - acc: 0.507 - 1s 38us/step - loss: 1.0585 - acc: 0.5076\n",
      "Epoch 2/3\n",
      "14464/23164 [=================>............] - ETA: 2s - loss: 1.1418 - acc: 0.468 - ETA: 1s - loss: 0.9926 - acc: 0.551 - ETA: 1s - loss: 0.9684 - acc: 0.560 - ETA: 0s - loss: 0.9639 - acc: 0.574 - ETA: 0s - loss: 0.9698 - acc: 0.566 - ETA: 0s - loss: 0.9693 - acc: 0.564 - ETA: 0s - loss: 0.9724 - acc: 0.563 - ETA: 0s - loss: 0.9765 - acc: 0.560 - ETA: 0s - loss: 0.9790 - acc: 0.561 - ETA: 0s - loss: 0.9833 - acc: 0.558 - ETA: 0s - loss: 0.9839 - acc: 0.5584"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_out = open(MODEL, \"wb\")\n",
    "\n",
    "pickle.dump(model, model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_in = open(MODEL,\"rb\")\n",
    "\n",
    "model = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the test set to 1's and 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_pred == y_pred.max(axis=1)[:,None]).astype(int)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
